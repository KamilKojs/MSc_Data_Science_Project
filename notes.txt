baseline:
    Loading the models to GPU and keeping them on GPU after the request, so if the same model is requested
    next time the request processing time is much faster
baseline2:
    unloading the model from GPU every time after request, increased logging
    baseline2 results file: second, workload_id, total inference time, model_to_gpu_time, inference_time, unload_from_gpu_time
baseline3:
    same as baseline but increased logging:
    Loading the models to GPU and keeping them on GPU after the request, so if the same model is requested
    next time the request processing time is much faster
    baseline3 results file: second, workload_id, total inference time, model_to_gpu_time, inference_time, unload_from_gpu_time
    In this case the sequence order is unload_from_gpu_time -> model_to_gpu_time
experiment1:
    using time series models and picking prediction with the highest probability,
    I actually picked prediction where any number in proba was the highest so
    it might have been wrong, I might have been picking preds that indicate negative proba...
experiment2:
    fixed the bug in experiment1 and rerun
experiment3:
    using time series models and picking prediction with the highest probability, increased logging
    experiment3 results file: second, workload_id, total inference time, workload_id_to_preload, model_on_gpu_before_preloading, manager_time_series_time, model_to_gpu_time, inference_time, unload_from_gpu_time
    In this case the sequence order is unload_from_gpu_time -> model_to_gpu_time
experiment4:
    the same as experiment 3 but loading 4 models into GPU at the same time, so serving 4 models simultaneously
    experiment4 results file: second, workload_id, total inference time, workload_id_to_preload_1, workload_id_to_preload_2, workload_id_to_preload_3, workload_id_to_preload_4, model_on_gpu_before_preloading_1, model_on_gpu_before_preloading_2, model_on_gpu_before_preloading_3, model_on_gpu_before_preloading_4, manager_time_series_time, model_to_gpu_time, inference_time, unload_from_gpu_time


next steps:
 - discuss on how much training data is needed to use such system
 - increase logging (each step separately), time breakdown, scenarios if model is kicked out in case of wrong prediction
 - more analysis on intuition on time series models performance, why hit miss increases time for prediction
 - experiment for multiple models on single GPU (or multiple GPU's), while still doing serving one by one

Experimentation framework:
 - hypothesis: implementation of manager component decreases average latency time for serving system
 - 11 cores - 10 for loading manager, 1 per each time series model, and 1 last core for serving server
 - RAM - undefined - as much as needed to load all ResNet152 models into RAM and run manager and server components
 - metric - latency for serving

 - baseline takes 10,7GB of RAM
 - nvidia-smi memory output for baseline: 1041MiB / 40960MiB
 - experiment 3 takes 12.1GB of RAM

 - experiment 4 takes 12.9GB of RAM
 - nvidia-smi memory output for experiment 4: 1749MiB / 40960MiB


Discussion and future work:
 - time series models performance Discussion
 - how much data is needed to train those time series models
 - retraining aspect over time
 - quality of service, discuss aspects of how much time each customer might want to wait for request processing


 - future work - check if similar results on different system traces (different datasets apart from Azure trace dataset)
 - future work - check how a system with multiple GPUs would perform - allows also for caching of unused models already on GPU
 